---
layout: single
title: "Beyond the Hype: A Technologist and Anthropologist Decode AI's Promise and Limits"
date: Mon Dec  1 04:16:13 PST 2025
last_modified_at: Mon Dec  1 05:22:55 PST 2025
permalink: /books/beyond-the-hype
categories:
 - book
tags:
 - ai
toc: true
toc_label: "&nbsp;Table of Contents"
toc_icon: "fa-solid fa-list"
toc_sticky: true
usemathjax: true
author_profile: true
---

posted: {{ page.date| date: "%d-%b-%Y" }}
&amp;
updated: {{ page.last_modified_at| date: "%d-%b-%Y" }}
{: .notice--primary}



# Prologue: The Question Behind the Algorithm

The email arrived at 6:47 AM on a Tuesday in December 2019, the subject line glowing in the pre-dawn darkness of my Vancouver home office:

"Q4 Impact Analysis - Mobile Shopping Recommendations."

I opened it with the mechanical reflex of someone who's checked email before morning coffee for twenty years. The body contained a single line and an Excel attachment:

"Recommendation system ROI confirmed: $207,133,371.12 incremental revenue, fiscal year 2019."

$207 million.

Two hundred and seven million dollars generated by algorithms I had architected, multi-armed bandits and collaborative filters dancing through millions of shopping sessions, predicting what people wanted before they knew they wanted it. The mathematics had been elegant—contextual embeddings capturing the ghost of consumer desire, Bayesian optimization tuning every parameter, and neural architectures that learned and adapted and *worked*.

God, it worked brilliantly.

I should have felt triumph. I should have forwarded the email to my team with celebration emojis. I should have opened champagne, called my family in Korea, posted something oblique on LinkedIn that would make my peers understand without being tacky about the numbers.

Instead, I sat there in the dark, watching the rain streak down the window, my coffee growing cold in a Stanford mug someone had given me a decade ago, feeling only a strange, expanding hollowness.

The house was silent except for the furnace clicking on somewhere in the family room. My wife and daughter were still asleep. The sky outside was that flat grey that makes December mornings in Vancouver feel like the world hasn't quite committed to existing yet. On my desk: two monitors, a mechanical keyboard worn smooth on the ASDF keys, and a framed photo of Stephen Boyd from my Stanford graduation.

I had optimized the objective function perfectly. The algorithms hummed. The revenue graphs climbed. Every metric exceeded, every quarterly review triumphant, every stakeholder satisfied. I had done exactly what Amazon hired me to do—taken twenty six years of mathematical training, from Seoul Science High School's Mathematics Olympiad rooms through Boyd's legendary Convex Optimization lectures at Stanford, through Samsung's semiconductor fabs where I'd revolutionized DRAM cell design, and applied it all to the vast machinery of digital commerce.

The math was impeccable. The execution was flawless. The impact was undeniable.
But somehow, watching that number glow on the screen, I understood with sudden clarity that I had answered the wrong question.

Not wrong mathematically. Not wrong technically. Not even wrong commercially.

But I knew it was *wrong* somehow.

---

I was fourteen the first time I encountered the right question though I didn't recognize it then.

It was 1990, late autumn in Seoul, the kind of evening when the heating hasn't quite caught up with the cold. I was alone in our apartment—the sounds of Gangnam's construction boom filtering up through double-paned windows that never quite sealed properly.

I sat at our upright piano, a modest Samick that my mother had scrimped to buy, practicing a Chopin Nocturne in E-flat major, Op. 9, No. 2. Nothing special. I'd been playing for years by then, with the methodical diligence of a good Korean student who practiced because practice was what you did, not because of any particular passion. My technique was adequate. My musicality was, charitably, developing.

But something happened that evening.

I can't tell you what measure I was in, or whether I played the passage particularly well. I remember only that a *feeling* washed over me—sudden, overwhelming, utterly beyond language. Not happiness exactly. Probably closer to bliss, but not that, either. Kind of peace, but not that, either. Something else, something that seemed to bypass words entirely and reach directly into whatever part of me experienced being alive probably somewhat existential part of me.

In that instant, I understood more clearly than I'd understood anything before why music is beautiful.

Not the theory—I'd studied that, could explain harmonic progressions and voice leading and cadential structure. Not the technique—I'd practiced scales until my fingers ached and knew how to execute a proper trill. But something else entirely. Something that lived in the space between the notes, in the resonance between mathematical structure and whatever-it-is that makes us human.

The nocturne's melody—those descending thirds in the right hand, the left hand's steady pulse—they were just air pressure variations. Sinusoidal waves propagating through space. One-dimensional acoustic phenomena that any microphone could measure. Yet this single-dimensional time-series data was giving me an experience richer than any visual art I'd seen, more emotionally profound than any literature I'd read, more moving than the mathematical theorems I'd studied. The paradox was impossible to ignore: the simplest possible sensory input—variations along a single dimension—was creating the most complex conscious experience I'd ever known.

I sat there in our Seoul apartment, the Chopin still resonating in my fingers, the streetlights beginning to glow orange through the window, and I didn't have an answer.

Thirty-five years later, with a PhD in electrical engineering, two decades building optimization and AI systems, hundreds of seminars delivered across the world's top universities, I still don't have an answer.

But I know the question matters.

I know it matters more than the $207 million.

---

Between that ninth-grade piano bench and the hollow morning in Vancouver lie decades of optimization.

At Samsung (2004-2017), I revolutionized how semiconductor engineers designed DRAM cells, moving the entire process from human intuition to mathematical proof. I remember the first time I showed the senior engineers at Samsung's Hwaseong campus my iOpt platform—their skepticism was palpable. They'd been designing chips longer than I'd been alive. Why would they trust an algorithm over their instincts?

But the math didn't lie. Hundreds of parameters—transistor widths, oxide thicknesses, doping concentrations—all optimized simultaneously using optimization techniques. What took human designers weeks of trial and error, my system could do in hours, and with provably optimal results. Within two years, hundreds of engineers were using iOpt daily. The satisfaction was real: watching abstraction become silicon, watching mathematical theory produce actual circuits that went into smartphones in millions of pockets worldwide.

At Stanford (1999-2004), Prof. Stephen Boyd taught me that convex optimization isn't just mathematics—it's a way of seeing structure in chaos, of recognizing when problems have a hidden elegance that makes solutions not just possible but inevitable. I remember sitting in his office in Packard Building, second floor, with the afternoon light streaming through, as he sketched an optimization landscape on his whiteboard. "See?" he said, in that understated way of his. "The problem *wants* to be solved. We just have to see its structure clearly."

My academic lineage traces back through Boyd to Carl Friedrich Gauss himself—through Fourier and Laplace, through Klein and Lipschitz, through mathematical truths that transcend culture, language, era. The same harmonics that Pythagoras discovered in vibrating strings underlie quantum mechanics, general relativity, and the neural networks powering today's AI revolution.

Mathematical inevitabilities. Truths that would hold in all imaginable universes.

But Amazon's $207 million taught me what inevitabilities cannot do: they cannot tell you what to optimize *for*.

---

The recommendation algorithms worked because they had a clear objective function: maximize revenue per session. Everything else followed mathematically. But "maximize revenue" is itself a choice, not a truth. We could have optimized for user satisfaction, for serendipitous discovery, for expanding rather than narrowing taste, for long-term customer loyalty, for helping people find what they *needed* rather than what they *wanted*, for human flourishing by some definition we'd have to actually articulate and defend.

The mathematics doesn't choose. The mathematics doesn't care. The mathematics is a powerful servant and a terrible master.

We choose. But mostly, we don't choose consciously. We inherit objective functions from quarterly earnings reports, from venture capital term sheets, from the ambient ideology that whispers constantly: *growth, scale, efficiency, and more*.

I sat there that December morning, the rain still falling, the $207 million still glowing on my screen, and I understood something that my fourteen-year-old self had intuited at the piano but that I'd spent two decades trying not to think about:

**The algorithm had taught me only how to pursue it efficiently. But the piano—long before I knew what algorithm meant—had taught me to ask what's worth pursuing.**

And the second question is infinitely harder than the first. But perhaps the only ultimately important question to ask!

---

This is a book about that second question—and whether we're asking it at all.

It begins where I am now: Co-Founder & CTO of Erudio Bio in California, building AI-powered cancer diagnostics that could actually save lives. Co-Founder & CEO of Erudio Bio Korea, bridging Silicon Valley innovation with Korean medical excellence. Using dynamic force spectroscopy—literally measuring the forces between individual molecules—and machine learning to detect biomarkers that might catch cancer years before traditional methods. Gates Foundation backing. Clinical trials with Seoul National University Bundang Hospital. Real medical impact. Stakes that couldn't be higher.

But also: watching Sam Altman testify to Congress about needing *trillions* of dollars and multiple nuclear power plants to scale AI. Watching these systems hallucinate with terrifying confidence—making up citations, inventing facts, and confabulating entire biographies. Watching the inequality gap widen as those with access to computation pull away from those without. Watching energy consumption soar toward environmental crisis while we call it progress. Watching tech billionaires become richer than nation-states while median wages stagnate.

And watching almost everyone—journalists, policymakers, investors, even many AI researchers—fundamentally misunderstand what these systems actually are.

---

Let me be precise about this, because precision matters:

**Large Language Models (LLMs) don't "know" anything.** They don't (and can’t) believe, reason, understand, or think. They estimate conditional probabilities over token sequences. They are extraordinarily good at pattern completion, and pattern completion can be extraordinarily useful. But completion is not comprehension. Statistical correlation is not causal understanding. Fluency is not consciousness. And most dangerously: confidence is not truth; never truth.

When ChatGPT tells you "Mary Lee Pfeiffer" in response to "Who is Tom Cruise's mother?" it isn't accessing knowledge. It's doing something far more mechanical yet paradoxically more remarkable: estimating conditional probabilities over token sequences.

The proof isn't just that it might get the reverse query wrong—it's that the asymmetry itself is impossible if genuine knowledge is present. When I ask the system "Who is Mary Lee Pfeiffer's son?" and it responds "I don't have specific information about Mary Lee Pfeiffer or her family," we're witnessing not a gap in knowledge but the fundamental nature of pattern completion. Knowledge is bidirectional. Pattern matching is not.

This isn't a bug. It's not a limitation we'll be able to overcome with more parameters or better training. It's *what these systems are*. It's their architecture, their ontology, and their fundamental nature.

Contemporary LLMs are too powerful, too versatile, and too useful for most people to accept that they fundamentally lack the human capacities we so readily attribute to them. Yet understanding what they truly do—and don't do—is crucial for our technological future.

The careless use of terms like "believes," "thinks," or "knows" isn't harmless shorthand. It actively shapes public understanding, policy decisions, and the trajectory of technological development. When lawmakers hear that AI systems "believe" things, they make fundamentally different regulatory decisions than they would if they understood these systems as sophisticated pattern-matching engines. When companies anthropomorphize their AI systems, they make different choices about trust, verification, and human oversight.

And it matters—*crucially*—that we understand this clearly. Not to diminish the achievement (these systems are remarkable), but to avoid catastrophic misdeployment. To avoid building a future on misunderstood foundations. To avoid optimizing brilliantly for the wrong things.

---

I'm writing this book with a co-author who brings everything I cannot: Jun-Young Park earned his PhD in cultural anthropology studying how humans create meaning in community, earned a master's degree in economy, and bachelor’s degree in semiconductor engineering (yes, really—he understands both the circuits and the culture), works as a writer translating complexity into clarity, and creates YouTube contents that make technical concepts accessible without dumbing them down.

Where I see algorithms, he sees rituals. Where I see optimization landscapes, he sees power structures. Where I see objective functions, he asks: whose objectives, and who chose them?

He asks the questions different than those engineers are trained to ask: *What does this technology change about being human? Whose values are encoded in "neutral" systems? When Silicon Valley talks about "disruption," who bears the cost? What are we optimizing for, really?*

Together, we're attempting something rare: a book that explains how AI *actually works*—with real technical precision with mathematical rigor, not metaphors or mysticism—while examining what it *means* for human societies, cultures, relationships, and futures.

Each chapter pairs technical analysis with anthropological inquiry. Semiconductor optimization meets Salzburg Global Seminars on inequality. Convex optimization meets the dimensional paradox of music. Stanford PhD rigor meets Seoul Science High School wonder. Amazon's machine learning meets the question: *Who decided this is what we should optimize for?*

Not another AI hype book promising miracles.

Not another AI doom book predicting apocalypse.

But an honest reckoning with both genuine capabilities and essential limitations—and the wisdom to tell them apart.

**Part One** follows my journey from Boyd's Convex Optimization theory through Samsung's fabs, Amazon's algorithms, Gauss Labs' industrial AI, to Erudio Bio's cancer diagnostics—not (only) as success stories but as laboratories for understanding what transfers across domains and what doesn't. What works, what fails, where AI creates value, and where it destroys it. Where pattern recognition should end and human judgment must begin.

**Part Two** dismantles the mythology. What LLMs actually do (and cannot do). Why they hallucinate; it’s by design (accidently), not accident. Why most digital transformations fail; it's humans, not technology. How AI amplifies inequality. Why "AI consciousness" is a category mistake. Why data, not models, is the real competitive advantage. What multi-agent systems might offer.

We refuse the false choice between Silicon Valley's techno-optimism and the humanities' techno-skepticism. Both perspectives are necessary. Both are insufficient alone.

<!--
---

The central tension of this book lives in that gap between the fourteen-year-old discovering ineffable beauty in mathematical ratios and the engineer watching $207 million in revenue flow from optimized recommendation systems.

Music taught me that consciousness can extract infinite richness from finite mathematical structures. That the same frequency ratios discovered by Pythagoras 2,500 years ago still move us to tears—not because of the mathematical inevitability, but because of what happens when the mathematical inevitability meets embodied, temporal, conscious experience in specific cultural contexts.

AI is teaching me the opposite lesson: that extraordinary technical capability without conscious experience produces something powerful but fundamentally alien. Pattern completion without comprehension. Fluency without understanding. Optimization without wisdom.

The question isn't whether AI will surpass human intelligence.

The question is whether we understand what human intelligence *is* (or rather what types of intelligence we should be pursuing)—and whether we'll build a future that amplifies or diminishes it.
-->

---

I'm writing this from Silicon Valley, where I lead the Silicon Valley Privacy-Preserving AI Forum (K-PAI), where Korean innovation meets American entrepreneurship, where I advise companies and governments on both continents, where I give dozens of seminars annually across Stanford, KAIST, Seoul National University, Samsung, POSTECH, DGIST, Sogang University, Yonsei University, and Korea University.

I've seen the venture capital pitches that promise "AI transformation" without understanding what the AI actually does or what transformation actually means. I've seen the quarterly pressure that turns thoughtful engineering into rushed deployment. I've seen the demo-to-production gap that kills 70% of AI projects. I've built systems that created genuine value and systems that created only PowerPoint slides. I've watched brilliant engineers confuse pattern matching with reasoning, correlation with causation, and deployment with understanding.

And I've watched inequality accelerate in real time. Watched energy consumption soar toward crisis. Watched truth become harder to distinguish from confident hallucination. Watched the gap widen between those who understand these systems and those who fear or worship them. Watched the concentration of computational power create new forms of geopolitical leverage. Watched developing nations forced to choose between Chinese surveillance systems and American profit extraction.

Mostly, I've watched us forget to ask: *What are we optimizing for?*

Not "Can we build this?" but "Should we?"

Not "Does it work?" but "For whom?"

Not "Is it intelligent?" but "Is it wise?"

Not "Does it scale?" but "Does it serve human flourishing?"

<!--
---

A year ago, I spent a week at Mozart's birthplace in Salzburg as part of the Salzburg Global Leadership Initiative, a fellowship that brought together leaders from around the world to grapple with technology, growth, and inequality. One evening, I played Mozart's Piano Sonata K. 545 on an aged grand piano in a 300-year-old building, the same building where Mozart himself had walked, the same city where he had composed, the same mathematical ratios that he had discovered echoing through my fingers.

The same harmonic series. The same frequencies that have resonated through conscious beings for millennia. The same 3:2 ratio that creates a perfect fifth whether you're in 18th-century Salzburg or 21st-century Silicon Valley or ancient Greece.

But here's what struck me with sudden force: **AI cannot touch our souls.**

Not because it lacks sophistication—current systems are extraordinarily sophisticated, more powerful than I could have imagined even a decade ago. But because they lack the integration of mathematical structure with conscious, embodied, temporal, cultural, lived experience that makes music *music* rather than mere acoustic patterns. That makes a perfect fifth *meaningful* rather than just a frequency ratio.

This isn't mysticism. It's mechanics. It's understanding the difference between processing patterns and experiencing meaning. Between completing sequences and comprehending significance. Between optimization and purpose. Between predicting the next token and knowing what matters.

The AI can analyze Mozart. It can generate Mozart-like compositions that pass Turing tests. It can even "understand" in some technical sense what makes Mozart Mozart, at least in terms of statistical patterns.

But it cannot be *moved* by Mozart. It cannot feel the longing in the slow movement, the wit in the allegro, the resolution that comes home to the tonic like a question answered. It cannot integrate the mathematics with memory, with loss, with joy, with the knowledge of mortality that makes beauty precious.

And that difference matters more than almost anything else we could focus on right now.
-->

---

This book is my attempt—our attempt—to articulate that difference clearly, and to argue that the difference matters for every decision we're making about AI right now.

Because we are building systems of tremendous power. And we are doing so while fundamentally misunderstanding what they are, what they can do, what they cannot do, and what we should even want them to do.

We are optimizing at planetary scale without asking what we're optimizing *for*.

We are deploying systems that can pass the bar exam and hallucinate case law in the same conversation.

We are trusting pattern completion with decisions that require judgment.

We are confusing statistical correlation with causal understanding.

We are mistaking fluency for knowledge, confidence for truth, capability for wisdom.

My ninth-grade self, sitting at that Samick piano in Seoul, overwhelmed by beauty that mathematics could describe but never fully explain, knew something my Amazon algorithms never will:

**The question is everything.**

The question behind the algorithm. The objective behind the optimization. The values behind the metrics. The purpose behind the power.

Let's get the question right.

---

**[Transition to co-author's voice]**

When Sunghee first told me about the Tom Cruise's mother problem—how ChatGPT confidently invents a biography for someone who doesn't exist—I recognized it immediately. This is every anthropologist's nightmare and vindication simultaneously.

Nightmare because millions now trust these systems with truth.

Vindication because it reveals what anthropology has always known: *knowledge is not data retrieval.*

I study how humans create meaning in community, across time, through ritual and relationship and the ineffable space between words. When machines simulate these processes without participating in them, something essential is lost—and something dangerous is created.

A cultural anthropologist and an optimization theorist writing about AI together might seem strange. But the strangeness is the point. What we're building in Silicon Valley requires both lenses: the technical and the human, the mathematical and the cultural, the analytical and the experiential.

This book is our attempt to hold both simultaneously. To refuse the false choice. To insist that the most important questions about AI are simultaneously technical and human.

How we answer those questions will determine whether AI amplifies human flourishing or undermines it. Whether it narrows inequality or widens it. Whether it preserves what makes us human or erodes it.

The technology is here. The question is what we do with it.

Let's begin.

*Sunghee Yun & Jun-Young Park*

*Silicon Valley & Seoul, December 2025*

# PART I: THE REAL AI JOURNEY — From Mathematics to Medicine

*Subtitled: How Algorithms Actually Get Applied (And What Works)*

## Chapter 1: The Foundation — Stanford, Boyd, and the Mathematics of Optimization

The late afternoon sun streamed through the tall windows of Packard Building's second floor, illuminating dust motes that hung in the air like tiny mathematical points suspended in space. It was winter 1999, my first academic year at Stanford, when the apprenticeship I'd hoped for became real: Prof. Boyd had accepted me as his PhD student. I sat across from him in his corner office, watching him sketch an optimization landscape on his whiteboard with practiced strokes.

"See?"

he said, in that understated way of his that made profound insights sound almost casual.

"The problem *wants* to be solved. We just have to see its structure clearly."

He drew a bowl-shaped curve—a simple parabola in two dimensions that represented something far more complex: a convex optimization problem in high-dimensional space. Then, for contrast, he sketched a landscape with multiple valleys and peaks, a treacherous terrain where algorithms could get trapped in local minima, forever missing the global optimum hidden somewhere in the distance.

"General optimization," he said, tapping the multi-valley landscape, "is like searching for the deepest point in a mountain range while blindfolded. You might find a valley, but how do you know it's the deepest valley? You don't. You can't. Not without checking every other valley, which might take forever."

He turned back to the bowl. "But convex optimization?" A slight smile. "Every valley you find *is* the deepest valley. Every local minimum *is* the global minimum. It's not just easier—it's categorically different."

I had come to Stanford to study electrical engineering, drawn by its legendary faculty and the convergence of theory and practice that Silicon Valley represented. But I hadn't expected to encounter something that felt less like a technique and more like a revelation about the architecture of solvable problems.

The mathematics was elegant in a way I'd rarely experienced. **When a problem is convex, every path downward leads to success.** Not to "a" solution, but to "*the*" solution—provably, globally, optimally.

It was 1999. The dot-com bubble was about to burst leaving many parts of Silicon Valley littered with the detritus of failed startups and abandoned business plans. But in Boyd's office, surrounded by stacks of papers and the quiet hum of mathematical certainty, I was learning something that would outlast every boom and bust cycle: some problems have an intrinsic structure that makes them not just solvable, but *guaranteeably* solvable.

---

### The Miracle of Convexity

To understand why this matters—not just for academic mathematics but for everything from the circuits in your smartphone to the algorithms that recommend your next purchase—you need to grasp what convexity actually *means*.

A function is convex if, when you draw a line between any two points on its graph, the function lies below that line. Geometrically, this means no "bumps" that could trap you. Analytically, it means that the *local* structure of the problem tells you everything about its *global* structure.

Picture it this way: imagine you're hiking in thick fog, unable to see more than a few feet ahead, trying to find the lowest point in the terrain. In a general (non-convex) landscape, following the slope downward might lead you into a shallow valley while the true lowest point lies somewhere else, hidden by ridges you can't see. You'd need to exhaustively explore every depression, every valley, every dip—a potentially infinite task.

But in a convex landscape—a perfect bowl shape, mathematically speaking—any path downward eventually leads to the same place: the global minimum. The local gradient points toward global truth. Following the slope is *sufficient*. You need no cleverness, no intuition, no luck. The structure of the problem itself guides you to the answer.

This isn't just convenient. **It's transformative.**

Consider the general optimization problem:

$$
\begin{array}{ll}
\text{minimize} & f_0(x) \\
\text{subject to} & f_i(x) \leq 0, \quad i = 1, \ldots, m \\
& h_j(x) = 0, \quad j = 1, \ldots, p
\end{array}
$$

In the general case, this problem can be extraordinarily difficult. Local search algorithms might get trapped. Determining global optimality can be computationally intractable—sometimes impossible in practice, even for moderately sized problems.

But when $f$ and $g_i$ are convex functions, and $h_j$ are affine (linear) functions, everything changes.

$$
\begin{array}{ll}
\text{minimize} & f_0(x) \\
\text{subject to} & f_i(x) \leq 0, \quad i = 1, \ldots, m \\
& Ax = b
\end{array}
$$

The feasible region—the set of points satisfying all constraints—becomes convex. The objective function has no spurious local minima. And most remarkably: **any local minimum is automatically a global minimum.**

This guarantee transforms the impossible into the systematic. It turns art into science.

---

### Not an Accident—A Search for Mathematical Beauty

I'd loved mathematics obsessively since childhood—winning medals at the Korean Mathematics Olympiad, insisting on proving every theorem in my engineering textbooks in college while classmates just wanted to use the formulas. When I arrived at Stanford in 1998 planning to research digital communication (the hot field then, like AI is now), I thought I knew my path.

Then I took Boyd's EE364: Convex Optimization in Winter 1999.

The course was legendarily difficult, but I wasn't frustrated—I was home. Late one night, working through a problem set that used duality theory to reveal hidden structure in a circuit design, something clicked. Not just understanding the problem—understanding what I wanted to do with my life.

I emailed Boyd. The next day, he accepted me into his research group.

Everything that followed—Samsung, Amazon, Gauss Labs, Erudio Bio—traces back to that moment when a search for mathematical beauty found its home.

<!--
People sometimes ask how I ended up studying convex optimization. They assume it was strategic planning or career calculation. It wasn't. But it also wasn't accident or serendipity.

It was a search that finally found its destination.

I'd loved mathematics since childhood. In high school, I won medals at the Korean Mathematics Olympiad and the Asian-Pacific Mathematics Olympiad, spending weekends proving theorems while my classmates played basketball. When it came time to choose a college major, I wanted to study pure mathematics—to spend my life in that world of rigorous proofs and elegant abstractions.

My father and high school teacher had other ideas. "You can do mathematics in electrical engineering, too," they argued. "But EE gives you something more—lots of critical twentieth-century technology as we know it started from the invention and commercialization of semiconductors. Everything modern derives from that. You'll have both the mathematics you love and the practical impact you need."

They were persuasive. I enrolled in Electrical Engineering (EE) at Seoul National University (SNU).

But my obsession with mathematical rigor persisted—to a degree that probably annoyed my professors. In Engineering Mathematics, where the entire purpose was to *use* theorems as tools, I insisted on *proving* every theorem in the textbook. Other students wanted to know "how do I apply this?" I wanted to know "why must this be true?"

Yet another peculiar thing about me: I loved coding. Not the way most software developers love it—not for building applications or solving practical problems—but for its structural beauty. The way elegant code reveals mathematical patterns. The way a well-designed algorithm expresses abstract logic in concrete form. Looking back, I loved coding for the same reason I loved mathematics: both revealed beautiful structures hiding beneath surface complexity.

When I applied to Stanford, my statement of purpose declared I wanted to research digital communication. This was 1998—digital communication was the hot field, almost exactly the way AI is now. Everyone wanted to work on it. The market demanded it. It seemed like the obvious choice.

Digital communication was among the most mathematical topics in electrical engineering (far more so than, say, semiconductor device physics). But when I started taking digital communication classes at Stanford, I felt something missing. The mathematics was there, yes—but it wasn't *deep* enough. It felt applied, instrumental, a means to an end rather than an end in itself.

During the first quarter at Stanford, in Autumn 1998, I took EE263: Introduction to Linear Dynamical Systems.

Boyd's course.

I fell in love immediately.

The way he presented material—terse, precise, and yet extremely practical while demanding that you see not just techniques but the *structure* beneath them. The way every lecture built toward deeper understanding of how linear systems behave, why certain problems have elegant solutions, what mathematical principles govern dynamic behavior.

This was mathematics the way I'd always wanted to experience it: rigorous, beautiful, and simultaneously practical.

When I started taking EE364: Convex Optimization in Winter quarter 1999, something crystallized.

The course was legendarily difficult. Problem sets required not just calculation but genuine insight. I spent entire weekends in Cecil H. Green Library, surrounded by other sleep-deprived graduate students, wrestling with duality theory and KKT conditions.

But somehow I wasn't frustrated. It didn’t take long before I realized I was *home*.

I still remember one particular night—early in the quarter, probably 2 AM, almost right after I passed the PhD qualifying exam, working through a problem set in my dormitory. The problem involved formulating a circuit design question as a convex optimization problem, then using duality theory to reveal hidden structure in the solution.

As I worked through it, something clicked. Not just understanding the problem—*understanding what I wanted to do with my life.*

This—*this*—was the research I'd been searching for all along without knowing exactly what I was searching for. The perfect synthesis of rigorous mathematics and practical application. Pure enough to satisfy my obsession with proof and structure. Applied enough to matter for real engineering problems.

The feeling was overwhelming. Certainty. Conviction. Destiny, if you believe in such things.

I opened my email and wrote to Prof. Boyd.

The next day, I visited his office.

"What do you want to work on?"

"I don't know yet," I admitted. "But I want to learn how to see the way you see."

That understated smile again. "That's a good reason."

He accepted me into his research group. Just like that! Two quarters had not passed since I started pursuing advanced degrees at Stanford.

No long deliberation. No strategic career planning. Just the recognition—on both sides, I think—that I'd found where I belonged.

The course remained difficult. Boyd's expectations remained demanding. But I was finally doing mathematics the way I'd always wanted: deep, rigorous, beautiful, and useful.

Everything that followed—Samsung, Amazon, Gauss Labs, Erudio Bio—traces back to that late night and that email. To the moment when a search for mathematical beauty found its home in convex optimization.

Not accident. Not serendipity.

Just a long-held love finally meeting its proper object.

Boyd taught us to recognize when problems possessed hidden convex structure—and when we could transform non-convex problems into equivalent convex ones through clever reformulation.

It was like learning a secret language that certain problems spoke. Once you heard it, you couldn't unhear it. Suddenly, problems that had seemed impossibly complex revealed themselves as special cases of more general convex formulations. Linear programming, quadratic programming, semidefinite programming—these weren't separate techniques but instances of a unified framework.

Convex optimization reveals its full power through duality theory. If convexity is the heart of optimization theory, duality is its soul.

"The primal asks: what is the smallest value satisfying these constraints?" Boyd says. "The dual asks: what is the best lower bound we can construct using these constraints? Strong duality tells us these questions have the same answer. That's not obvious. That's *remarkable*."

It *was* remarkable. And in that moment, I understood that I wasn't just learning techniques. I was learning to see mathematical structure—the architecture of solvable problems, the hidden elegances that make certain questions answerable.
-->

### The First-Order Optimality Condition

One of the most beautiful results in convex optimization is also one of the simplest. For a differentiable convex function, a point $x^\ast$ is optimal if and only if

$$
\nabla f_0(x^*) = 0
$$

That's it. Find where the gradient vanishes, and you've found the global minimum. No need to check second derivatives. No need to explore the rest of the space. The local condition is sufficient for global optimality.

This is *remarkable* when you think about it. In general optimization, a zero gradient tells you only that you've found a critical point—which could be a local minimum, a local maximum, or a saddle point. You'd need to check second derivatives (the Hessian matrix) and even then you'd only know about local behavior.

But for convex functions, the first-order condition is complete. The local gradient tells you everything about the global structure.

I remember the first time I truly grasped this. Boyd had assigned a problem about portfolio optimization—choosing how to allocate money across different assets to maximize expected return while minimizing risk. The problem had a convex formulation (a quadratic objective with linear constraints), which meant we could solve it efficiently even with thousands of assets.

But what struck me wasn't the efficiency—it was the *certainty*. Once we found the solution, we *knew* it was globally optimal. Not just "probably good" or "best we could find"—actually, provably, globally optimal.

In a world of uncertainty, heuristics, and approximations, this kind of guarantee felt almost magical.

---

### Duality: The Hidden Perspective

If convexity is the heart of optimization theory, duality is its soul.

Every optimization problem has a dual problem—a hidden perspective that often provides deeper insights than the original formulation. For the primal problem:

$$
\begin{array}{ll}
\text{minimize} & f_0(x) \\
\text{subject to} & f_i(x) \leq 0, \quad i = 1, \ldots, m \\
& h_j(x) = 0, \quad j = 1, \ldots, p
\end{array}
$$

we construct the Lagrangian:

$$
L(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{j=1}^p \nu_j h_j(x)
$$

The dual function is then:

$$
g(\lambda, \nu) = \inf_{x} L(x, \lambda, \nu)
$$

And the dual problem seeks the best lower bound (by maximizing its objective function):

$$
\begin{array}{ll}
\text{maximize} & g(\lambda, \nu) \\
\text{subject to} & \lambda \geq 0
\end{array}
$$

**Weak duality** tells us the dual optimal value always provides a lower bound on the primal optimal value. This always holds (even for non-convex problems).

But for convex problems satisfying mild regularity conditions (like Slater's condition), we have **strong duality**: the dual optimal value equals the primal optimal value.

This is profound. It means every convex optimization problem can be viewed from two perspectives—the primal (minimizing the objective) and the dual (maximizing the best lower bound)—and these perspectives give the same answer.

The dual variables have beautiful interpretations. In economics, they're shadow prices—the marginal value of relaxing a constraint by one unit. In game theory, they're optimal strategies. In machine learning, they reveal which data points are most important (support vectors, for example).

Boyd devoted three full lectures to duality theory. "This isn't just a mathematical curiosity," he emphasized. "Duality gives you *another way to see the problem*. Sometimes the dual is easier to solve. Sometimes the dual has a clearer interpretation. Sometimes the dual reveals structure the primal obscures."

He was right. Years later at Samsung, I would rely on dual formulations to solve circuit optimization problems. But the real surprise came from an unexpected application: I could use dual optimal values to perform local sensitivity analysis of the primal problem. This theoretical insight became practically essential when designing next-generation DRAM cell schemes—it gave me exact derivatives of objective and constraint functions that would have been impossible to compute otherwise.

The irony was beautiful: I was *solving optimization problems to calculate derivatives*—inverting the usual logic where generally derivatives (are used to) solve optimization problems. Duality theory had revealed a hidden bridge between two seemingly separate mathematical tasks.

These moments—when abstract theory suddenly unlocked concrete engineering problems, when mathematical elegance coincided perfectly with practical necessity—were among the most astounding and gratifying of my career. They felt less like clever applications and more like glimpses of some deeper structure in the universe, where pure mathematics and physical reality were different expressions of the same underlying truth.

At Amazon, dual perspectives would illuminate what the primal obscured. When collaborative filtering algorithms failed mysteriously, the dual formulation exposed the actual bottleneck: not the algorithm design but the constraint structure. Dual variables quantified exactly how data sparsity limited performance—turning vague concerns about insufficient training data into precise measurements of marginal value. The same shadow prices that had optimized transistor widths at Samsung now optimized recommendation quality for millions of users.

At Erudio Bio, duality theory shapes our entire approach to biomarker discovery. Finding cancer signatures in biological data means optimizing detection accuracy under strict constraints: limited samples, measurement noise, biological variability. The primal asks what biomarker combinations achieve our accuracy targets. The dual asks what accuracy is actually achievable given our constraints—and which constraints limit us most. This perspective transforms experimental design: instead of trying random biomarker combinations hoping for accuracy improvements, we compute exactly which additional measurements would most relax our binding constraints. Pure mathematics, guiding the search for tools that might save lives."

### The KKT Conditions: Where Geometry Meets Analysis

The Karush-Kuhn-Tucker (KKT) conditions represent one of the most elegant bridges between geometry and analysis in all of mathematics.

For a point to be optimal in a convex optimization problem, it must satisfy:

**Stationarity:**

$$
\nabla f_0(x^\ast) + \sum_{i=1}^m \lambda_i^\ast \nabla f_i(x^\ast) + \sum_{j=1}^p \nu_j^\ast \nabla h_j(x^\ast) = 0
$$

**Primal feasibility:**

$$
f_i(x^\ast) \leq 0, \quad h_j(x^\ast) = 0
$$

**Dual feasibility:**

$$
\lambda_i^* \geq 0
$$

**Complementary slackness:**

$$
\lambda_i^\ast f_i(x^\ast) = 0
$$

These conditions capture the geometric intuition that at the optimum, the gradient of the objective function must be a linear combination of the gradients of the active constraints.

For convex problems, the KKT conditions are both necessary and sufficient for optimality. Find a point satisfying these conditions, and you've found the global optimum. This transforms constrained optimization from an art into a systematic science.

I remember working through a particularly tricky problem set where we had to use the KKT conditions to solve a portfolio optimization problem analytically. The problem involved maximizing expected return while constraining risk and requiring certain positions to be non-negative. After hours of algebra, the KKT conditions revealed something beautiful: at the optimum, assets split into three categories. Some had positive holdings (with zero dual variable—their non-negativity constraint wasn't binding). Some had zero holdings (with positive dual variable—they would enter the portfolio only if their expected return increased). And some had holdings exactly at their constraint boundaries.

The complementary slackness condition encoded this perfectly: for each constraint, either the constraint is satisfied with slack ($f_i(x^\ast) < 0$*) and its dual variable is zero (*$\lambda_i^\ast = 0$), or the constraint is active ($*f_i(x^\ast) = 0$) and its dual variable is positive (*$\lambda_i^\ast > 0$).

This single condition—$\lambda_i^\ast f_i(x^\ast) = 0$—captured the entire economics of constrained resource allocation.

Pure. Elegant. Universal.

### Interior-Point Methods: Revolution in Algorithms

The theoretical beauty of convex optimization would matter less if we couldn't actually *solve* these problems efficiently. This is where interior-point methods enter the story—a algorithmic revolution that occurred in the 1980s.

Traditional methods like the simplex algorithm for linear programming worked by walking along the edges of the feasible region. Interior-point methods took a radically different approach: they stay inside the feasible region and approach the boundary only in the limit.

The key insight is to replace hard inequality constraints $f_i(x) \leq 0$ with barrier functions that approach infinity as you approach the boundary:

$$
\text{minimize} \quad f(x) + \frac{1}{t} \sum_{i=1}^m -\log\left(-f_i(x)\right)
$$

As the parameter $t$ increases, solutions to these barrier problems trace out the "central path" that approaches the optimal solution of the original problem.

Using Newton's method to solve the barrier problems, and carefully increasing $t$, interior-point methods solve convex optimization problems in polynomial time—a theoretical breakthrough that also had enormous practical impact.

Boyd's textbook, *Convex Optimization* (co-authored with Lieven Vandenberghe), came out in 2004, just as I was finishing my PhD. It became the Bible of the field—the text researchers quoted chapter and verse, the book PhD students annotated like theological scholars. Mathematically rigorous yet surprisingly readable, with applications spanning engineering, finance, statistics, and machine learning, it shaped how an entire generation understood not just optimization techniques but the very structure of solvable problems.

That book is now cited over 84,000 times. It shaped how an entire generation of researchers and practitioners think about optimization. And it emerged from the same tradition Boyd was teaching me: **mathematical precision in service of practical problem-solving.**

### Why This Powers Modern AI

Fast forward twenty six years from those afternoon sessions in Boyd's office. Today, virtually every machine learning system relies on the foundations we were studying in EE364.

Training a neural network? You're solving a (usually non-convex, but locally convex) optimization problem.

Fitting a support vector machine to data? Convex optimization. Period.

Training a logistic regression, which is a simplest form of neural network? Convex optimization. Period.

Solving a linear regression? Convex optimization with a closed-form solution. This happens every time you draw fitting lines in your Excel sheets.

Designing recommendation systems, detecting spam, recognizing images, translating languages—all of these involve solving optimization problems, many of which have convex structure or convex relaxations.

The $207 million in revenue my Amazon recommendation systems generated? That came from iteratively solving hundreds of extremely large convex optimization problems per second, each of them guaranteeing we could find good solutions efficiently.

Google's PageRank algorithm that revolutionized web search? Based on the principal eigenvector of a matrix—a problem with convex formulation.

The training of GPT-4 and every other large language model? Massive-scale optimization problems, broken into smaller pieces with convex structure.

**Convex optimization is the backbone of modern AI.** It's the hidden infrastructure that makes machine learning actually *work* at scale.

But—and this matters enormously—the fact that we can *efficiently find* optimal parameters for a machine learning model tells us nothing about whether the model should exist, what it should optimize for, whose data it should train on, or what values it embodies.

We can train a model to maximize engagement (which often means maximizing outrage and addiction). We can train a model to maximize revenue (which often means exploiting psychological vulnerabilities). We can train a model to maximize accuracy on a benchmark (which often means encoding historical biases into automated systems).

The mathematics works beautifully for all of these. Convex optimization doesn't judge your objective function—it just guarantees you'll reach it efficiently.

And here's where modern AI gets even more complex: deep learning—the technology behind GPT-4, image recognition, and most headline-grabbing AI—actually involves *non-convex* optimization. The loss landscapes have multiple local minima. We have no guarantee that gradient descent finds the global optimum.

Yet even these non-convex problems rely fundamentally on convex optimization principles. We break massive problems into smaller convex subproblems. We use local convex approximations to guide optimization. We apply techniques—like momentum methods and adaptive learning rates—that emerged from studying convex optimization. The training of every neural network layer involves solving convex optimization problems iteratively.

So when people ask "How does AI actually work?" a significant part of the answer is: "Convex optimization—applied millions of times, at massive scale, using principles discovered in Boyd's office and classrooms like EE364."

The infrastructure is mathematical. The applications are human. And the gap between those two is where everything interesting—and dangerous—happens.

### Mathematical Lineage and Universal Truths

One afternoon in his office, Boyd showed me his academic genealogy chart—the mathematical family tree tracing advisor-student relationships backward through history.

"You see this?" He pointed to his name, then traced upward through his advisor, and their advisor, and back through generations of mathematicians. The line went through Carl Friedrich Gauss himself, then further back through Fourier and Laplace, through Klein and Lipschitz, all the way to the very foundations of mathematical analysis.

My Erdős number—the degrees of separation from the legendary mathematician Paul Erdős through coauthorship—was three. But this was different. This was intellectual lineage, ideas passing through minds across centuries, each generation adding insights while preserving the core truths.

"These aren't just names," Boyd said. "These are people who discovered things that are true in *all possible universes*. The Pythagorean theorem. Fourier analysis. The structure of convex sets. These truths existed before humans evolved and will exist after we're gone. We're just the temporary custodians of these insights."

He paused, looking at the chart. "That's what mathematics is—the search for universal truths. The things that would have to be true regardless of physics, biology, culture. The inevitable structures."

I thought about this often over the years. The same mathematical inevitabilities that Pythagoras discovered in vibrating strings 2,500 years ago underlie quantum mechanics, general relativity, and the neural networks powering today's AI revolution. The harmonic series that creates music's beauty is the same series that appears in signal processing, wireless communications, and machine learning.

**Mathematical truths are universal.** They transcend culture, language, era. A theorem proven in ancient Greece remains valid in modern Silicon Valley. An algorithm that works follows mathematical principles that would work on any planet with rational beings capable of computation.

But here's what Boyd never explicitly said, though I eventually understood: **mathematics can tell you *how* to optimize, but it cannot tell you *what* to optimize for.**

The objective function—that thing we're trying to minimize or maximize—comes from outside mathematics. From human choice. From values, goals, purposes we must articulate and defend.

Convex optimization gives you guaranteed success reaching your objective. But it offers no guidance on whether that objective is worth reaching.

### Finding My Place in the Lineage

That genealogy chart Boyd showed me—the line from him to Gauss—it represented more than intellectual history. It represented something I'd been searching for since childhood without knowing quite what I was searching for.

I'd been obsessed with mathematics from an early age. Not the casual "I'm good at math" that many students experience, but something closer to addiction. In high school, I won medals at the Korean Mathematics Olympiad and the Asian-Pacific Mathematics Olympiad, spending weekends working through proofs while my classmates played basketball or pursued more typical teenage activities.

The beauty of mathematics consumed me—the way a proof would suddenly click into place, the elegance of a well-constructed argument, the inevitability of logical deduction. Other subjects felt arbitrary, contingent, and debatable. Mathematics felt true in a way nothing else did.

When it came time to choose a college major, I knew exactly what I wanted: pure mathematics. I wanted to spend my life in that world of rigorous proofs and elegant abstractions, following arguments wherever they led.

My father and my high school teacher had other ideas.

"You can do mathematics in electrical engineering, too," they argued almost unanimously.
"But EE gives you something more," my teacher continued. "Most critical twentieth-century technology as we know it started from the invention and commercialization of semiconductors. The transistor, integrated circuits, and almost everything modern derives from that. You'll have both the mathematics you love and the practical impact you need."
My father nodded. "Mathematics is beautiful, but what will you do with it? Teach? EE opens more doors."

They were persuasive. And perhaps I wasn't as certain as I thought. I enrolled in Electrical Engineering at Seoul National University.

But my obsession with mathematical rigor didn't fade—it just frustrated my engineering professors.
In Engineering Mathematics—a course designed to teach engineers how to use mathematical tools, not exactly to make them understand their theoretical foundations—I insisted on proving every theorem in the textbook. Every. Single. One.

Other students would ask: "How do I apply Fourier transforms to this circuit problem?"
I would ask: "But why must the Fourier transform have these properties? Can we prove convergence rigorously?"
The professor would sigh. "Yun, this is an engineering course. We don't have time for all the proofs. Just learn to use the tools."

I couldn't. I needed to know why things were true, not just that they worked.
Looking back, I realize I was trying to smuggle pure mathematics into engineering through the back door.

But there was something else, too—something that would eventually lead me to optimization rather than pure mathematics.

I loved coding.

Not the way most software developers loved it—not (only) for building applications or solving practical problems—but (also) for its structural beauty. The way elegant code revealed mathematical patterns. The way a well-designed algorithm expressed abstract logic in concrete form. The way you could see the architecture of a solution in the organization of the code itself.

I loved coding for the same reason I loved mathematics: both revealed beautiful structures hiding beneath surface complexity. A good algorithm was like a good proof—economical, clear, and inevitable.

This should have told me something about what I was really looking for. Not pure mathematics divorced from application, but mathematics embodied in systems, in algorithms, in things that computed and solved and worked.

When I applied to Stanford in 1998, I stated in my Statement of Purpose that I wanted to research digital communication. This was the hot field at the time—almost exactly the way AI is now. Everyone wanted to work on it. The market demanded it. The future seemed to lie in wireless systems, information theory, and coding theory.

It seemed like the obvious choice. Digital communication was among the most mathematical topics in electrical engineering—far more so than, say, semiconductor device physics or power systems. It involved beautiful theory: information theory, probability, signal processing, and coding theory.

I arrived at Stanford in Summer 1998 ready to immerse myself in this world.

But something felt wrong.

The mathematics was there, yes—but it wasn't deep enough for me. It felt applied, instrumental, a means to an end rather than an end in itself. The problems were interesting, but they didn't scratch the itch I'd had since those Korean Mathematics Olympiad days.

Then, in that autumn quarter, I took EE263: Introduction to Linear Dynamical Systems.
Boyd's course.
Everything changed.

The way he presented material—terse, precise, practical, yet demanding that you see not just techniques but the structure beneath them. The way every lecture built toward deeper understanding of how systems behave, why certain problems have elegant solutions, what mathematical principles govern dynamic behavior.

This wasn't just applied mathematics. This was mathematics itself—rigorous, beautiful—that happened to be applicable.

I fell in love immediately.

When I enrolled in EE364: Convex Optimization in the next quarter, Winter 1999, that love deepened into certainty.

The course was legendarily difficult. Problem sets required not just calculation but genuine insight. I spent entire weekends in Cecil H. Green Library, surrounded by other sleep-deprived graduate students, wrestling with duality theory and KKT conditions.

But I wasn't frustrated. Within a few weeks, (and probably deep inside, immediately) I realized: I was home.

I still remember the night it crystallized completely.
It was early in the quarter, probably 2 AM, not long after I'd passed the PhD qualifying exam. I was in my dormitory, working through a problem set alone. The problem involved formulating a circuit design question as a convex optimization problem, then using duality theory to reveal hidden structure in the solution.

As I worked through the dual formulation, computing the Lagrangian, deriving the dual function, something extraordinary happened.

The dual problem revealed information that was completely hidden in the primal formulation. It showed exactly which constraints were limiting the solution, quantified how much the objective would improve if we relaxed each constraint, exposed the fundamental trade-offs in the circuit design.

It was like seeing in a new dimension. The same problem, but viewed from a completely different (but obviously connected) angle that revealed structure I couldn't see before.

And in that moment—2 AM, alone in my room, staring at pages of mathematical derivations—I suddenly understood what I wanted to do with my life.

This. This perfect synthesis of rigorous mathematics and practical application. This field where proving theorems mattered and solving real problems mattered. This framework elegant enough to satisfy my obsession with mathematical beauty, yet powerful enough to design circuits, optimize systems, solve engineering problems that actually mattered.

Pure enough for the mathematician in me. Applied enough for the engineer also inside me!

The feeling was overwhelming. Certainty. Conviction. Destiny, if you believe in such things.

I didn't wait until morning. I couldn't.
I did what I had to do.
I opened my email and wrote to Prof. Boyd right then, at 2 AM:

"Professor Boyd,

I would like to work with you on convex optimization.
I don't know yet exactly what research problems or topics I want to pursue, but I know this is the field where I belong. Would you consider taking me as your PhD student?"

I hit send and went to sleep.

The next day, Boyd replied: "Come by my office this afternoon."

I sat across from him in that same office where, months later, he would sketch optimization landscapes on his whiteboard. Where he would show me the mathematical genealogy chart. Where I would spend countless hours over the next five years learning to see the world through optimization's lens.

"What do you want to work on?" he asked.

"I don't know yet," I admitted. "But I want to learn how to see the way you see—to recognize structure, to know when problems can be solved, to understand the architecture of solvable problems."

That understated smile. "That's a good reason."

He accepted me into his research group. Just like that.

Two quarters. That's all it took from arriving at Stanford to finding my life's work. Not because I'd planned it strategically, but because I'd finally encountered the thing my obsessive mathematical mind had been searching for all along.

My father and teacher had been right, in the end. Electrical engineering did give me mathematics and practical impact. Just not in digital communication, not in the field I'd expected.
But in the field I had never imagined would exists somewhere in this world.

I remembered when I told Prof. In-Joong Ha in SNU that I wanted to understand (or rather know) what the positive-definiteness of matrices mean.
Prof. Ha told me that that's exactly one of many things that you would learn in graduate school.
I hoped so badly that I would have that understanding once I arrive at Stanford.
He was right, but wrong, too.
He was right in that I finally had that understanding in Prof. Boyd's Convex Optimization class.
He was (kind of) wrong in that that happened in a totally unexpected way.

<span id="aa"></span>
In Boyd's office, working on convex optimization, I found my place in that lineage stretching back to Gauss
(who not only called Number Theory as the Queen of Mathematics, but also devised a very practical mathematical method
called Least-Square Methods, which is literally still being used in every modern mathematical tools including Excel sheets every day today).
Not as a pure mathematician proving theorems divorced from application. Not as a pure engineer building systems divorced from deep theory.
But as something in between—someone who could see mathematical structure and build things that mattered. Someone who needed both the beauty of proofs and the satisfaction of systems that worked.

The fourteen-year-old who fell in love with Chopin's Nocturne had learned that beauty can justify existence by itself. The Stanford graduate student working through duality theory at 2 AM learned that beauty can also serve—that elegance and utility need not be separate.

That mathematical lineage Boyd showed me wasn't just about intellectual pedigree. It was about finding where you belong—where your particular obsessions and capabilities align with problems that matter.

Everything that followed—Samsung, Amazon, Gauss Labs, Erudio Bio—would flow from that 2 AM moment of recognition.

Not accident. Not serendipity.

Just a long-held love finally meeting its proper object.


### What Stanford Taught Me Beyond Mathematics

Looking back, my years with Boyd taught me three things that shaped everything that followed:

**First: Structure matters more than cleverness.**

The most powerful solutions come not from ingenious tricks but from recognizing the inherent structure of problems. When you see that a problem is convex—or can be transformed into something convex—you unlock guaranteed global solutions. No amount of cleverness helps if you're solving the wrong class of problem.

This lesson would prove crucial at Samsung, where I'd transform circuit design from an art (experienced engineers tweaking parameters based on intuition) into a science, or further into technology (algorithms exploiting convex structure to prove optimality).

**Second: Local information can reveal global truth—but only in special cases.**

For convex problems, following the local gradient leads to global optima. This is remarkable but also *rare*. Most problems aren't convex. Most landscapes have multiple valleys. And for those problems, local information is insufficient.

This lesson would haunt me at Amazon, where we'd build systems that optimized locally (maximizing click-through rates, optimizing engagement metrics) without asking whether local optima were globally desirable.

**Third: Mathematics tells you *how* but not *what*.**

Convex optimization gives you guaranteed success reaching any objective you specify. But it offers no guidance on whether that objective is worth reaching. The objective function comes from outside mathematics—from values, goals, purposes you must articulate and defend.

This lesson I'm still learning. It's the gap between the fourteen-year-old at the piano (overwhelmed by beauty, asking *why* this matters) and the engineer watching $207 million flow from optimized algorithms (understanding *how* it works but questioning *what* it's for).

---

### Leaving Stanford, Carrying Questions

In 2004, I defended my PhD dissertation: "Convex Optimization for Digital Integrated Circuit Applications." The work was solid—I'd developed new algorithms for digital circuit sizing using new class of convex optimization called generalized geometric program to handle more complex circuit constraints.

The defense went smoothly. My committee asked good questions. Boyd nodded at my answers with that understated approval I'd learned to recognize. I was done.

That evening, I walked through Stanford's campus—past the Rodin sculptures, past Memorial Church with its golden mosaics, past the palm trees swaying in the California breeze. In my bag was a copy of my dissertation, 162 pages of theorems, algorithms, proofs, and most of all, applications.

I had learned to see mathematical structure. I could recognize convexity, construct duality, exploit optimization landscapes. I could prove theorems and write code and solve problems that would have been intractable without these tools.

But I carried questions too—questions that mathematics couldn't answer:

*What should we optimize for?*

*When is efficiency good, and when does it destroy something valuable?*

*How do we choose objectives that serve human flourishing rather than arbitrary metrics?*

*What happens when we optimize brilliantly for the wrong things?*

These questions would follow me to Samsung, where algorithms would meet silicon. To Amazon, where theory would generate hundreds of millions in revenue. To Gauss Labs, where industrial AI would confront manufacturing reality. To Erudio Bio, where optimization would serve human health.

---

### Anthropological Lens: The Culture of Silicon Valley Academia and the Mythology of Mentorship

*By Jun-Young Park*

Sunghee's account of his time with Stephen Boyd reveals something anthropologists recognize immediately: the ritual construction of legitimate knowledge through lineage.

When Boyd showed Sunghee the mathematical genealogy chart tracing back to Gauss, he was performing what we call *tradition authentication*—establishing that their work participates in a continuous chain of intellectual authority stretching back centuries. This isn't unique to mathematics or Stanford. You see it in martial arts (tracing lineages back to legendary masters), in religions (apostolic succession), in crafts (master-apprentice relationships).

But Silicon Valley academia adds its own distinctive flavor to this ancient pattern.

**The Mythology of Individual Genius**

Notice how the story centers on individual relationships: student to advisor, advisor to their advisor, brilliant mind to brilliant mind. This narrative erases the broader social infrastructure that makes such relationships possible—the staff who maintain the department, the taxpayer funding that supports research, the thousands of students who don't get mentored by famous professors, the wives and families who supported these "great men" throughout history.

The genealogy chart is a technology of prestige. It tells you who *matters* (those with famous advisors) and who doesn't (those working in industry, those at less prestigious institutions, those whose contributions were intellectual but not "original" enough for naming credit).

When Sunghee says having Boyd as his advisor was "one of the luckiest things that have ever happened to my life," he was being genuine—this relationship genuinely shaped his career. But the flip side is that career opportunities in academia and elite industry disproportionately accrue to those who win the advisor lottery. The system rewards those who study with professors who combine rare intellectual gifts with outstanding scholarly records, creating self-reinforcing prestige hierarchies.

**Legitimacy Through Lineage vs. Legitimacy Through Results**

Contrast this with Silicon Valley's *other* mythology—the scrappy entrepreneur who drops out of Stanford to build something that "changes the world." That narrative prizes results over pedigree, disruption over tradition.

These two mythologies coexist in Stanford's culture: you're supposed to *learn from established masters* (gaining legitimacy through lineage) while *disrupting existing order* (gaining legitimacy through innovation). The contradiction is rarely examined.

Sunghee navigated both tracks. His PhD provided institutional legitimacy—the credential of Boyd's lineage that opened doors at Samsung, Amazon, Gauss Labs, and eventually enabled him to co-found a Gates Foundation-supported biotech company building AI platforms for drug discovery. But his real impact came from bridging theory and practice: transforming abstract optimization principles into working circuit designs, recommendation systems, and biomarker discovery tools.

**The Invisible Labor of Knowledge Transfer**

Boyd's teaching appears effortless in Sunghee's account—the understated comments, the simple diagrams, the elegant insights. But this effortlessness is itself a performance, a display of mastery that obscures enormous preparation and accumulated experience.

Anthropologists studying expertise recognize this pattern: true masters make difficult things look easy, which paradoxically makes their expertise seem almost *mystical* rather than learned through deliberate effort. This mystification serves the prestige economy—if Boyd's insights seem to flow from natural genius rather than decades of practice, they become more valuable and less replicable.

The real knowledge transfer happens not in those elegant office sessions but in the hundreds of hours Sunghee spent working through problem sets, reading papers, failing at solutions, gradually developing mathematical intuition. The mythology focuses on the advisor's wisdom. The reality is mostly the student's work.

**Universal Truth vs. Cultural Practice**

Sunghee writes about mathematical truths being "universal—transcending culture, language, and era." And in one sense, this is correct. The Pythagorean theorem holds in all contexts. Convex optimization principles work regardless of who applies them.

But *what* we choose to optimize, *which* problems we consider worth solving, *how* we deploy these mathematical tools—these are deeply cultural choices.

The fact that a generation of researchers now frame diverse problems as optimization problems reflects not just the power of the mathematics but the cultural dominance of a particular way of seeing. We've been trained to ask: "How can I formulate this as an optimization problem?" We less often ask: "Should this be optimized at all?"

Consider: indigenous knowledge systems often resist optimization logic. Many traditional practices seem "inefficient" from an optimization perspective—rituals that take longer than necessary, agricultural methods that yield less than maximized, social customs that constrain individual choice. But these "inefficiencies" often encode hard-won wisdom about sustainability, social cohesion, and values beyond maximization.

When we export optimization thinking globally—teaching it in universities worldwide, embedding it in AI systems deployed across cultures—we're not just spreading neutral mathematical tools. We're spreading a particular worldview about what matters (that which can be quantified, measured, and optimized) and what doesn't (that which resists quantification).

**The Question Boyd Couldn't Teach**

Sunghee ends the chapter noting that mathematics tells you *how* to optimize but not *what* to optimize for. This is precisely where technical expertise meets cultural values—and where anthropology becomes essential.

Optimization is never value-neutral. Every objective function embodies choices about what matters. When Amazon optimizes for engagement, it chooses to value attention over well-being. When drug companies optimize for profit, they choose shareholder returns over patient access. When we optimize AI systems for benchmark performance, we choose measurable metrics over harder-to-quantify harms.

The mathematical optimization succeeds perfectly in all these cases. The algorithms find global optima. The KKT conditions are satisfied. Duality holds.

But whose values are we optimizing for? Who decides the objective function? Who benefits from the optimization, and who bears the costs?

These questions can't be answered by mathematics alone. They require what mathematics deliberately excludes: context, power relations, historical understanding, ethical reasoning, and democratic deliberation.

**The Sacred and the Secular**

Finally, notice the almost religious language (but actually farthest from religious, and rather extremely logical, according to his accounts) Sunghee uses about mathematical truth: "universal truths," "inevitable structures," "things that would have to be true in all possible universes (if there could be such things)."

This isn't accidental. Mathematics occupies a curious position in modern culture—it carries some of the authority that religious truth once held. Mathematical proofs are *certain* in ways almost nothing else can be. They promise access to eternal truths independent of human judgment or cultural variation.

But this sacredness has consequences. When we formulate problems mathematically, we *purify* them—stripping away the messy social context that makes them matter. A mathematical optimization problem is clean, formal, and pure. The real-world application is messy, political, and value-laden.

The danger is using mathematical purity to avoid engagement with messy reality. "We're just optimizing this objective function" becomes a way to avoid asking whether the objective is just. "The algorithm made this decision" becomes a way to disclaim responsibility for the decision's consequences.

Sunghee doesn't fall into this trap—his prologue explicitly names the gap between mathematical elegance and human values. But many who learn these tools do. They believe that making something mathematical makes it neutral, objective, and beyond critique (wrongly).

Understanding convex optimization is essential for working with modern AI systems. But understanding the cultural work that optimization thinking *does*—how it shapes what questions we ask, what solutions we consider legitimate, what values we embed in systems—is equally essential.

The mathematics is universal. What we do with it is not.

## Chapter 2: Samsung — When AI Met Silicon (2004-2017)

- Circuit optimization: Optimizing hundreds of parameters simultaneously
- The DRAM cell design revolution: From human intuition to mathematical proof
- Building iOpt: A rebellious ML platform in a Galaxy-dominated workplace
- Real impact: Tools still used by 200+ engineers daily
- **[Anthropological lens: Corporate culture, resistance to change, and the rituals of engineering organizations]**

## Chapter 3: Amazon — The $200 Million Lesson in Recommender Systems (2017-2020)

- Moving from semiconductors to e-commerce: What transfers and what doesn't
- How recommendation algorithms actually create value (not magic, math)
- The Mobile Shopping App project: Architecture, implementation, impact
- Why deep learning isn't always the answer
- **[Anthropological lens: Consumer behavior, digital desire, and the invisible algorithms shaping modern commerce]**

## Chapter 4: Gauss Labs — Industrial AI and the Bayesian Reality Check (2020-2023)

- Co-founding SK Group's first AI company: The Silicon Valley-Korea bridge
- When Bayesian methods outperform deep learning: Real examples from manufacturing
- Why "AI transformation" promises often fail: The gap between demo and deployment
- Building AI for real factories, not PowerPoint presentations
- **[Anthropological lens: The clash between innovation rhetoric and operational reality; the human element in "smart" factories]**

## Chapter 5: Erudio Bio — AI Meets the Complexity of Life (2023-Present)

- From circuits to cells: Why biotech is AI's next frontier
- AlphaFold, Bio-TCAD, and the data scarcity wall in medicine
- Fighting cancer with AI-powered biomarker discovery
- The Gates Foundation grant: Validation and responsibility
- **[Anthropological lens: The culture of medicine, the ethics of AI in healthcare, and who benefits from biological breakthroughs]**

## Interlude: The Pattern Emerges

*A reflection on what these five stages reveal about AI's real capabilities and limits*

---

## ART II: DEMYSTIFYING AI — What It Is, What It Isn't, and What It Means

*Subtitled: The Truth Behind the Hype (And Why It Matters)*

## Chapter 6: What LLMs Actually Do — The Conditional Probability Truth

- Opening the black box: Transformers, attention mechanisms, and pattern completion
- The Tom Cruise's mother example: Why LLMs don't truly "know" anything
- Why ChatGPT can sound smart without understanding a single word
- The architecture of illusion: How statistical correlation mimics comprehension
- **[Anthropological lens: Why humans are so easily fooled by linguistic fluency; the anthropomorphization trap]**

## Chapter 7: AI Cannot Believe, Reason, Know, or Think

- The philosophical precision we need: Defining knowledge, belief, reasoning
- Why "chain of thought" is pattern matching, not reasoning
- The consciousness question: Category errors in the AI debate
- Mozart's birthplace and the soul AI cannot touch
- **[Anthropological lens: Cultural constructions of intelligence, consciousness across societies, and why Western categories may mislead us]**

## Chapter 8: The Hallucination Problem — Not a Bug, a Feature

- Why LLMs hallucinate: It's baked into the architecture
- Confidence without knowledge: The most dangerous combination
- Real examples from high-stakes applications (healthcare, legal, financial)
- Can we fix it? Technical and philosophical limitations
- **[Anthropological lens: Truth, trust, and authority in the age of confident AI misinformation]**

## Chapter 9: AI Fairness, Bias, and the Inequality Accelerator

- The Salzburg Global Seminar: Technology, growth, and inequality
- Why AI systems inherit and amplify human biases
- Facial recognition, hiring algorithms, and systemic injustice
- The Hamburg Declaration and global governance challenges
- Who benefits from AI advancement? Who pays the cost?
- **[Anthropological lens: Power structures, technological colonialism, and whose values get encoded into "neutral" systems]**

## Chapter 10: The Energy Crisis Nobody Talks About

- LLMs' staggering energy consumption: The invisible cost
- Why Sam Altman wants trillions and nuclear fusion
- Liquid Neural Networks and the efficiency imperative
- Sustainability vs. scale: Can we have both?
- **[Anthropological lens: Modern society's relationship with energy, growth ideology, and the mythology of limitless progress]**

## Chapter 11: Why Digital Transformation Fails — The Human Factor

- The 70% failure rate: Why most AI projects don't deliver
- Technology vs. culture: When algorithms meet organizational reality
- The "not invented here" syndrome and other human resistances
- Change management, training, and the soft skills nobody teaches engineers
- Real stories from Samsung, Amazon, and Gauss Labs
- **[Anthropological lens: Organizational culture, power dynamics, resistance to change, and the rituals that shape technology adoption]**

## Chapter 12: Data is Destiny — The Real Competitive Advantage

- Why models are commoditizing but data isn't
- RAG, vector databases, and domain-specific knowledge
- The future belongs to those who organize knowledge, not those who build bigger models
- Building data moats in the age of AI agents
- **[Anthropological lens: Knowledge as cultural capital; who owns information in digital societies]**

## Chapter 13: Multi-Agent AI — The Next Paradigm

- From single models to agent societies
- Collective intelligence vs. monolithic systems
- Privacy-preserving AI and the K-PAI vision
- Real applications: Where multi-agent systems shine
- **[Anthropological lens: Parallels to human social organization; what agent cooperation reveals about collective intelligence]**

## Chapter 14: The Healthspan Revolution — AI's Most Important Application

- Why living longer isn't enough: The healthspan challenge
- Erudio Bio's mission: From cancer detection to health asset management
- AlphaFold, personalized medicine, and biological breakthroughs
- The ethical imperative: Ensuring access for all
- **[Anthropological lens: Cultural attitudes toward aging, death, health across societies; medicalization and its discontents]**

---

## PILOGUE: Optimizing for Humanity

## Chapter 15: The Future We Choose — Technology, Wisdom, and Values

- AI that amplifies human capabilities, not replaces human agency
- The Rhône River reflection: Technology carrying human values forward
- What the next generation of entrepreneurs should build
- Bridging Silicon Valley innovation and global equity
- The irreplaceable value of human creativity, consciousness, and wisdom
- **[Final anthropological reflection: What makes us human in an age of intelligent machines; cultural evolution and technological change]**

---

## PPENDICES

**Appendix A:** Glossary of Technical Terms (for non-technical readers)

**Appendix B:** Mathematical Foundations — A Primer on Convex Optimization and Neural Networks

**Appendix C:** Resources for Further Learning (Books, Papers, Courses)

**Appendix D:** The K-PAI Framework — Building Responsible AI Communities

**Appendix E:** Interview Guide — Questions for Evaluating AI Solutions in Your Organization
